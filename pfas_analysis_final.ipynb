{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast PFAS Data\n",
    "\n",
    "Goal: The goal of this project is to train a model to forecast future levels of PFAS contamination\n",
    "in different regions using historical PFAS data. The PFAS data collected by the EPA spans\n",
    "multiple years and regions, and students will use this data to predict future PFAS levels.\n",
    "Specifically, students will train models using data from 2001 to 2023 to forecast PFAS levels in\n",
    "2024. The objective is to explore how well models can predict future growth or changes in PFAS\n",
    "levels based on historical trends.\n",
    "\n",
    "Data url: https://www.epa.gov/dwucmr/occurrence-data-unregulated-contaminant-monitoring-rule#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using directory: /Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/ucmr5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Determine root directory based on OS\n",
    "if sys.platform == \"win32\":  # Windows\n",
    "    root_dir = r\"C:\\Users\\kdubf\\OneDrive\\projects\\cs_6463_final\\ucmr5\"\n",
    "else:  # macOS\n",
    "    root_dir = \"/Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/ucmr5\"\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    raise Exception(f\"Directory not found: {root_dir}\")\n",
    "\n",
    "print(f\"Using directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading UCMR5_AddtlDataElem.txt...\n",
      "Shape: (298135, 7)\n",
      "--------------------------------------------------\n",
      "Reading UCMR5_All.txt...\n",
      "Shape: (948284, 24)\n",
      "--------------------------------------------------\n",
      "Reading UCMR5_ZIPCodes.txt...\n",
      "Shape: (30304, 2)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_txt_to_df(file_path):\n",
    "    \"\"\"Helper function to read txt file into dataframe with consistent settings\"\"\"\n",
    "    # Read the header to get column names\n",
    "    headers = pd.read_csv(file_path, sep='\\t', encoding=\"ISO-8859-1\", nrows=0).columns\n",
    "    \n",
    "    # Create a dictionary to specify all columns as string type\n",
    "    dtype_dict = {col: str for col in headers}\n",
    "    \n",
    "    # Read the full file with string dtypes\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        sep='\\t',\n",
    "        encoding=\"ISO-8859-1\",\n",
    "        dtype=dtype_dict,\n",
    "        keep_default_na=False,  # Prevent pandas from converting certain strings to NaN\n",
    "        na_filter=False  # Don't interpret any values as NA/NaN\n",
    "    )\n",
    "\n",
    "# Read each file into its own dataframe\n",
    "try:\n",
    "    print(\"Reading UCMR5_AddtlDataElem.txt...\")\n",
    "    df_addtl = read_txt_to_df(os.path.join(root_dir, 'UCMR5_AddtlDataElem.txt'))\n",
    "    print(f\"Shape: {df_addtl.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading UCMR5_AddtlDataElem.txt: {str(e)}\")\n",
    "    df_addtl = None\n",
    "\n",
    "try:\n",
    "    print(\"Reading UCMR5_All.txt...\")\n",
    "    df_all = read_txt_to_df(os.path.join(root_dir, 'UCMR5_All.txt'))\n",
    "    print(f\"Shape: {df_all.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading UCMR5_All.txt: {str(e)}\")\n",
    "    df_all = None\n",
    "\n",
    "try:\n",
    "    print(\"Reading UCMR5_ZIPCodes.txt...\")\n",
    "    df_zip = read_txt_to_df(os.path.join(root_dir, 'UCMR5_ZIPCodes.txt'))\n",
    "    print(f\"Shape: {df_zip.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading UCMR5_ZIPCodes.txt: {str(e)}\")\n",
    "    df_zip = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify All Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CollectionDate to datetime and MRL & AnalyticalResultValue to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out lithium records from df_all. Lithium is neither PFAS nor does it correlate to any PFAS.\n",
    "df_all = df_all[df_all['Contaminant'] != 'lithium']\n",
    "\n",
    "# Convert CollectionDate to datetime\n",
    "df_all['CollectionDate'] = pd.to_datetime(df_all['CollectionDate'])\n",
    "\n",
    "# Create a new column CollectionDate_FOM that is the first of the month and year of CollectionDate\n",
    "df_all['CollectionDate_FOM'] = df_all['CollectionDate'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Create a new column that is the 4-digit year and 2-digit month of CollectionDate\n",
    "df_all['CollectionDate_YYYYMM'] = df_all['CollectionDate'].dt.strftime('%Y%m').astype(int)\n",
    "\n",
    "# Convert MRL to numeric, replacing non-numeric values with NaN\n",
    "df_all['MRL'] = pd.to_numeric(df_all['MRL'], errors='coerce')\n",
    "\n",
    "# Convert AnalyticalResultValue to numeric, replacing non-numeric values with NaN and then filling NaNs with 0\n",
    "df_all['AnalyticalResultValue'] = pd.to_numeric(df_all['AnalyticalResultValue'], errors='coerce').fillna(0)\n",
    "\n",
    "# Add key field to df_all using 'PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode' with \"_\" separator\n",
    "df_all['key'] = df_all[['PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode']].apply(lambda x: '_'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary fields\n",
    "\n",
    "These fields are identifiers, duplicatative information, empty, or have one-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PWSName, Size, FacilityName, SamplePointName, SamplePointType, AssociatedFacilityID \n",
    "# AssociatedSamplePointID, SampleID, MonitoringRequirement, UCMR1SampleType\n",
    "df_all.drop(columns=['PWSName', 'Size', 'FacilityName', 'SamplePointName', 'SamplePointType', \n",
    "                     'AssociatedFacilityID', 'AssociatedSamplePointID', 'SampleID', 'MonitoringRequirement', \n",
    "                     'UCMR1SampleType'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create records for each key, contaminant, and collectiondate_yyyymm, ensuring the existing records aren't altered. Only missing records are added by forward and backward checking \n",
    "\n",
    "Run-time for this section is ~30 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 915941\n",
      "Records after deduplication: 915722\n",
      "Records after filling: 16461216\n",
      "New records added: 15545494\n",
      "Original records in output: 915722\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_records(df_all, start_date='202301', end_date='202406'):\n",
    "    \"\"\"\n",
    "    Fill in missing monthly records using a date spine while respecting unique\n",
    "    contaminants per key. Optimized version with duplicate handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_all : pandas.DataFrame\n",
    "        Input dataframe containing key, Contaminant, and date columns\n",
    "    start_date : str\n",
    "        Start date in YYYYMM format\n",
    "    end_date : str\n",
    "        End date in YYYYMM format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Filled dataframe with complete monthly records and is_new flag\n",
    "    \"\"\"\n",
    "    # Create date range\n",
    "    date_range = pd.date_range(\n",
    "        start=pd.to_datetime(start_date, format='%Y%m'),\n",
    "        end=pd.to_datetime(end_date, format='%Y%m'),\n",
    "        freq='MS'\n",
    "    )\n",
    "    \n",
    "    # Deduplicate records\n",
    "    df_deduped = (df_all.sort_values('CollectionDate')\n",
    "                       .drop_duplicates(['key', 'Contaminant', 'CollectionDate_FOM'], \n",
    "                                      keep='first')\n",
    "                       .copy())\n",
    "    \n",
    "    # Store original records\n",
    "    original_records = df_deduped[['key', 'Contaminant', 'CollectionDate_FOM']].copy()\n",
    "    \n",
    "    # Get unique key-contaminant pairs\n",
    "    key_contam_pairs = df_deduped[['key', 'Contaminant']].drop_duplicates()\n",
    "    \n",
    "    # Create cartesian product of key-contaminant pairs and dates\n",
    "    all_dates = pd.DataFrame({'CollectionDate_FOM': date_range})\n",
    "    key_contam_dates = (key_contam_pairs.assign(key_dummy=1)\n",
    "                                      .merge(all_dates.assign(key_dummy=1), on='key_dummy')\n",
    "                                      .drop('key_dummy', axis=1))\n",
    "    \n",
    "    # Merge with original data to get all combinations\n",
    "    df_filled = (key_contam_dates.merge(df_deduped, \n",
    "                                      on=['key', 'Contaminant', 'CollectionDate_FOM'],\n",
    "                                      how='left'))\n",
    "    \n",
    "    # Forward fill and backward fill within groups\n",
    "    def fill_group(group):\n",
    "        # Select only the columns we want to fill\n",
    "        fill_cols = group.columns.difference(['key', 'Contaminant', 'CollectionDate_FOM'])\n",
    "        result = group.copy()\n",
    "        # Explicitly call infer_objects() to handle the downcasting warning\n",
    "        result[fill_cols] = (result[fill_cols]\n",
    "                            .ffill()\n",
    "                            .bfill()\n",
    "                            .infer_objects(copy=False))\n",
    "        return result\n",
    "\n",
    "    # Modified groupby operation to avoid column conflicts\n",
    "    df_filled = (df_filled.set_index(['key', 'Contaminant', 'CollectionDate_FOM'])\n",
    "                         .groupby(level=['key', 'Contaminant'], group_keys=False)\n",
    "                         .apply(fill_group))\n",
    "    \n",
    "    # Reset index without creating duplicate columns\n",
    "    df_filled = df_filled.reset_index()\n",
    "    \n",
    "    # Mark new records using merge\n",
    "    df_filled['is_new'] = ~df_filled[['key', 'Contaminant', 'CollectionDate_FOM']].apply(\n",
    "        tuple, axis=1).isin(original_records[['key', 'Contaminant', 'CollectionDate_FOM']].apply(\n",
    "        tuple, axis=1))\n",
    "    \n",
    "    # Update date columns\n",
    "    df_filled['CollectionDate'] = df_filled['CollectionDate_FOM']\n",
    "    df_filled['CollectionDate_YYYYMM'] = df_filled['CollectionDate'].dt.strftime('%Y%m').astype(int)\n",
    "    \n",
    "    print(f\"Original records: {len(df_all)}\")\n",
    "    print(f\"Records after deduplication: {len(df_deduped)}\")\n",
    "    print(f\"Records after filling: {len(df_filled)}\")\n",
    "    print(f\"New records added: {df_filled['is_new'].sum()}\")\n",
    "    print(f\"Original records in output: {(~df_filled['is_new']).sum()}\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Run the function\n",
    "filled_df = fill_missing_records(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Save or Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "save_dir = '/Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/modeling'\n",
    "\n",
    "# Make sure the directory exists\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames as pickle files\n",
    "# filled_df.to_pickle(os.path.join(save_dir, 'filled_df.pkl'))\n",
    "# df_all.to_pickle(os.path.join(save_dir, 'df_all.pkl'))\n",
    "\n",
    "# Load pkl files\n",
    "filled_df = pd.read_pickle(os.path.join(save_dir, 'filled_df.pkl'))\n",
    "# df_all = pd.read_pickle(os.path.join(save_dir, 'df_all.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prior observation (t-1) of AnalyticalResultValue, month, and season as predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by key, CollectionDate_YYYYMM, and Contaminant\n",
    "filled_df = filled_df.sort_values(['key', 'CollectionDate_YYYYMM', 'Contaminant'])\n",
    "\n",
    "# Create shifted variable grouped by the specified columns\n",
    "# Fill NA with the first value of each group (backfill)\n",
    "filled_df['AnalyticalResultValue_prior'] = (filled_df.groupby(['key', 'Contaminant'])['AnalyticalResultValue']\n",
    "                                          .shift(1))\n",
    "\n",
    "# Fill remaining NAs with the first value in each group\n",
    "filled_df['AnalyticalResultValue_prior'] = (filled_df.groupby(['key', 'Contaminant'])\n",
    "                                          ['AnalyticalResultValue_prior']\n",
    "                                          .transform(lambda x: x.fillna(x.iloc[0] if len(x) > 0 else 0)))\n",
    "\n",
    "# Add month feature (1-12)\n",
    "filled_df['month'] = filled_df['CollectionDate_YYYYMM'] % 100\n",
    "\n",
    "# Add season\n",
    "filled_df['season'] = pd.cut(filled_df['month'], \n",
    "                               bins=[0, 3, 6, 9, 12], \n",
    "                               labels=['Winter', 'Spring', 'Summer', 'Fall'], \n",
    "                               include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Additional Data Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table to get all possible columns\n",
    "all_columns_pivot = df_addtl.pivot_table(\n",
    "    index=['PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode'],\n",
    "    columns='AdditionalDataElement',\n",
    "    values='Response',\n",
    "    aggfunc=lambda x: list(x) if len(x) > 1 else x.iloc[0]\n",
    ").reset_index()\n",
    "\n",
    "# Find columns containing lists using map instead of applymap\n",
    "list_columns = all_columns_pivot.map(lambda x: isinstance(x, list)).any()\n",
    "columns_to_explode = list_columns[list_columns].index.tolist()\n",
    "\n",
    "result_df = all_columns_pivot\n",
    "for col in columns_to_explode:\n",
    "    result_df = result_df.explode(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all data, zip code data, and additional data elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the main data with additional elements\n",
    "final_df = filled_df.merge(\n",
    "    result_df,\n",
    "    on=['PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with ZIP codes\n",
    "# merged_df = final_df.merge(\n",
    "#     df_zip,\n",
    "#     on='PWSID',\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# Quick validation checks\n",
    "# print(f\"Original rows in df_all: {len(df_all)}\")\n",
    "# print(f\"Final merged rows: {len(final_df)}\")\n",
    "# print(f\"Number of unique PWSIDs: {final_df['PWSID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Save or Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "save_dir = '/Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/modeling'\n",
    "\n",
    "# Make sure the directory exists\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames as pickle files\n",
    "final_df.to_pickle(os.path.join(save_dir, 'final_df.pkl'))\n",
    "\n",
    "# Load pkl files\n",
    "# final_df = pd.read_pickle(os.path.join(save_dir, 'final_df.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding and remove unnecessary fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode', CollectionDate_FOM, CollectionDate, MRL, Units\n",
    "# AnalyticalResultsSign, State, is_new\n",
    "final_df.drop(columns=['PWSID', 'FacilityID', 'SamplePointID', 'SampleEventCode', 'CollectionDate_FOM', \n",
    "                       'CollectionDate', 'MRL', 'Units', 'AnalyticalResultsSign', 'State', 'is_new'], \n",
    "               inplace=True)\n",
    "\n",
    "\n",
    "# One-hot encode Contaminant, FacilityWaterType, MethodID, Region, CollectionDate_YYYYMM, DisinfectantType\n",
    "# LithiumOccurrence, LithiumTreatment, PFASOccurrence, PFASTreatment, PotentialPFASSources, PotentialPFASSourcesDetail\n",
    "# TreatmentInformation\n",
    "final_df_dmy = pd.get_dummies(final_df, columns=['Contaminant', 'FacilityWaterType', 'MethodID', 'Region', \n",
    "                                             'DisinfectantType', 'LithiumOccurrence', 'LithiumTreatment', 'PFASOccurrence', \n",
    "                                             'PFASTreatment', 'PotentialPFASSources', 'PotentialPFASSourcesDetail', \n",
    "                                             'TreatmentInformation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing datasets.\n",
    "\n",
    "Since the task is to predict PFAS levels for 2024, use 2023 data for training and 2024 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset (2023 data)\n",
    "tr_df = final_df_dmy[final_df_dmy['CollectionDate_YYYYMM'] // 100 == 2023].copy()\n",
    "tr_df = tr_df.dropna() \n",
    "tr_y = tr_df['AnalyticalResultValue']\n",
    "tr_x = tr_df.drop('AnalyticalResultValue', axis=1)\n",
    "\n",
    "# Create test dataset (2024 data)\n",
    "te_df = final_df_dmy[final_df_dmy['CollectionDate_YYYYMM'] // 100 == 2024].copy()\n",
    "te_df = te_df.dropna() \n",
    "te_y = te_df['AnalyticalResultValue']\n",
    "te_x = te_df.drop('AnalyticalResultValue', axis=1)\n",
    "\n",
    "# Drop CollectionDate_YYYYMM from both sets\n",
    "tr_x = tr_x.drop('CollectionDate_YYYYMM', axis=1)\n",
    "te_x = te_x.drop('CollectionDate_YYYYMM', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model with most important predictors and optimal hyperparameters\n",
    "\n",
    "Iterations of XGBoost model were separately performed to identify the optimal hyperparameters. To reduce run-time in the final version, the optimal hyperparameter values are fixed.\n",
    "\n",
    "The feature importance calculation is based on XGBoost's built-in feature importance mechanism, accessed through the feature_importances_ attribute. This attribute provides a numerical score for each feature that represents its contribution to model predictions.\n",
    "\n",
    "The feature selection process is implemented in the _select_features method and consists of the following steps:\n",
    "\n",
    "1. Initial Scoring   \n",
    "*Creates a DataFrame containing features and their importance scores   \n",
    "*Sorts features by importance in descending order\n",
    "\n",
    "2. Feature Filtering   \n",
    "*Implements a dual-criteria selection system:\n",
    "    *Region-based features: Automatically included regardless of importance    \n",
    "    *Non-region features: Selected based on importance threshold\n",
    "\n",
    "3. Threshold Application   \n",
    "*Uses the feature_threshold parameter (default: 0.02)   \n",
    "*Only non-region features exceeding this threshold are retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def evaluate_and_display_metrics(y_true, y_pred, dataset_name=\"\"):\n",
    "    \"\"\"Evaluate and display detailed metrics for model performance\"\"\"\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'Pearson_R': pearsonr(y_true, y_pred)[0]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:<10} {value:e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class PFASPredictor:\n",
    "    def __init__(self, feature_threshold=0.02):\n",
    "        self.feature_threshold = feature_threshold\n",
    "        self.model = None\n",
    "        self.important_features = None\n",
    "        self.label_encoders = {}\n",
    "        self.feature_combinations = None\n",
    "        \n",
    "        # Best parameters from previous optimization\n",
    "        self.best_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'n_estimators': 418,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.018391464127050988,\n",
    "            'min_child_weight': 3,\n",
    "            'subsample': 0.827156877063853,\n",
    "            'colsample_bytree': 0.9373610914155147,\n",
    "            'gamma': 0.06669347293641514,\n",
    "            'reg_alpha': 0.07382831113396796,\n",
    "            'reg_lambda': 0.002998108064038156,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "    def fit(self, tr_x, tr_y, te_x, te_y):\n",
    "        \"\"\"Train the model and store necessary information for future predictions\"\"\"\n",
    "        # Store keys separately\n",
    "        tr_keys = tr_x['key'].copy()\n",
    "        te_keys = te_x['key'].copy()\n",
    "        \n",
    "        # Store state and date information\n",
    "        self.states = sorted(list(set([k[:2] for k in tr_keys])))\n",
    "        \n",
    "        # Remove key from training features\n",
    "        tr_x_model = tr_x.drop('key', axis=1)\n",
    "        te_x_model = te_x.drop('key', axis=1)\n",
    "        \n",
    "        # Store the column names and their types\n",
    "        self.feature_types = tr_x_model.dtypes\n",
    "        \n",
    "        # Store unique feature combinations\n",
    "        self._store_feature_combinations(tr_x)\n",
    "        \n",
    "        # Preprocess the data and store label encoders\n",
    "        tr_x_processed = self._preprocess_data(tr_x_model, fit=True)\n",
    "        te_x_processed = self._preprocess_data(te_x_model, fit=False)\n",
    "        \n",
    "        # Log transform target variables\n",
    "        tr_y_log = np.log1p(tr_y)\n",
    "        te_y_log = np.log1p(te_y)\n",
    "        \n",
    "        # Create groups and compute weights\n",
    "        groups = tr_x.apply(lambda row: f\"{row['key']}_{self._get_contaminant(row)}\", axis=1)\n",
    "        weights = compute_sample_weight('balanced', tr_x_model.apply(self._get_contaminant, axis=1))\n",
    "        \n",
    "        # Train initial model and get feature importance\n",
    "        initial_model = xgb.XGBRegressor(**self.best_params)\n",
    "        initial_model.fit(tr_x_processed, tr_y_log, sample_weight=weights)\n",
    "        \n",
    "        # Select features\n",
    "        self.important_features = self._select_features(initial_model, tr_x_processed.columns)\n",
    "        \n",
    "        # Train final model with selected features\n",
    "        tr_x_selected = tr_x_processed[self.important_features]\n",
    "        te_x_selected = te_x_processed[self.important_features]\n",
    "        \n",
    "        self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        print(\"\\nTraining final model with selected features...\")\n",
    "        self.model.fit(\n",
    "            tr_x_selected, tr_y_log,\n",
    "            sample_weight=weights,\n",
    "            eval_set=[(te_x_selected, te_y_log)],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate_performance(self, tr_x, tr_y, te_x, te_y):\n",
    "        \"\"\"Evaluate model performance on training and test sets\"\"\"\n",
    "        # Prepare data\n",
    "        tr_x_processed = self._preprocess_data(tr_x.drop('key', axis=1), fit=False)\n",
    "        te_x_processed = self._preprocess_data(te_x.drop('key', axis=1), fit=False)\n",
    "        \n",
    "        tr_x_selected = tr_x_processed[self.important_features]\n",
    "        te_x_selected = te_x_processed[self.important_features]\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = np.expm1(self.model.predict(tr_x_selected))\n",
    "        test_pred = np.expm1(self.model.predict(te_x_selected))\n",
    "        \n",
    "        # Evaluate and display metrics\n",
    "        train_metrics = evaluate_and_display_metrics(tr_y, train_pred, \"Training Set\")\n",
    "        test_metrics = evaluate_and_display_metrics(te_y, test_pred, \"Test Set\")\n",
    "        \n",
    "        # Print feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.important_features,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        return train_metrics, test_metrics\n",
    "    \n",
    "    def evaluate_by_contaminant(self, te_x, te_y):\n",
    "        \"\"\"Evaluate model performance by contaminant\"\"\"\n",
    "        # Prepare test data\n",
    "        te_x_processed = self._preprocess_data(te_x.drop('key', axis=1), fit=False)\n",
    "        te_x_selected = te_x_processed[self.important_features]\n",
    "        \n",
    "        # Make predictions\n",
    "        test_pred = np.expm1(self.model.predict(te_x_selected))\n",
    "        \n",
    "        # Get contaminant for each row\n",
    "        contaminants = te_x.apply(self._get_contaminant, axis=1)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results = pd.DataFrame({\n",
    "            'contaminant': contaminants,\n",
    "            'true_values': te_y,\n",
    "            'predictions': test_pred\n",
    "        })\n",
    "        \n",
    "        # Evaluate by contaminant\n",
    "        contam_results = []\n",
    "        for contam, group in results.groupby('contaminant'):\n",
    "            metrics = evaluate_and_display_metrics(\n",
    "                group['true_values'], \n",
    "                group['predictions'],\n",
    "                f\"Contaminant: {contam}\"\n",
    "            )\n",
    "            contam_results.append({\n",
    "                'contaminant': contam,\n",
    "                'count': len(group),\n",
    "                'RMSE': metrics['RMSE'],\n",
    "                'R2': metrics['R2']\n",
    "            })\n",
    "        \n",
    "        contam_summary = pd.DataFrame(contam_results).set_index('contaminant').sort_values('RMSE')\n",
    "        print(\"\\nTest Set Performance by Contaminant:\")\n",
    "        print(contam_summary)\n",
    "        \n",
    "        return contam_summary\n",
    "    \n",
    "    def _preprocess_data(self, df, fit=False):\n",
    "        \"\"\"Preprocess data and handle label encoding\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].dtype.name == 'category':\n",
    "                if fit:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    df_processed[col] = self.label_encoders[col].fit_transform(df_processed[col])\n",
    "                else:\n",
    "                    df_processed[col] = self.label_encoders[col].transform(df_processed[col])\n",
    "                    \n",
    "        return df_processed\n",
    "    \n",
    "    def _store_feature_combinations(self, X):\n",
    "        \"\"\"Store unique feature combinations efficiently\"\"\"\n",
    "        # Extract features excluding key\n",
    "        features_df = X.drop('key', axis=1)\n",
    "        \n",
    "        # Get state from key\n",
    "        features_df['state'] = X['key'].str[:2]\n",
    "        \n",
    "        # Store unique combinations as tuples\n",
    "        self.feature_combinations = features_df.drop_duplicates().to_dict('records')\n",
    "    \n",
    "    def _get_contaminant(self, row):\n",
    "        \"\"\"Extract contaminant from feature columns\"\"\"\n",
    "        contam_cols = [col for col in row.index if 'Contaminant_' in col]\n",
    "        for col in contam_cols:\n",
    "            if row[col] == 1:\n",
    "                return col.replace('Contaminant_', '')\n",
    "        return None\n",
    "    \n",
    "    def _select_features(self, model, columns):\n",
    "        \"\"\"Select important features while keeping all Region predictors\"\"\"\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Select all Region features and only other important features\n",
    "        region_features = [col for col in columns if col.startswith('Region_')]\n",
    "        important_non_region = feature_importance[\n",
    "            (~feature_importance['feature'].isin(region_features)) & \n",
    "            (feature_importance['importance'] > self.feature_threshold)\n",
    "        ]['feature'].tolist()\n",
    "        \n",
    "        return region_features + important_non_region\n",
    "    \n",
    "    def generate_future_data(self, start_yearmonth=202407, end_yearmonth=202412):\n",
    "        \"\"\"Generate future prediction data for each state\"\"\"\n",
    "        # Convert yearmonth to string format\n",
    "        start_yearmonth = str(start_yearmonth)\n",
    "        end_yearmonth = str(end_yearmonth)\n",
    "        \n",
    "        # Generate all month combinations\n",
    "        years = range(int(start_yearmonth[:4]), int(end_yearmonth[:4]) + 1)\n",
    "        months = range(1, 13)\n",
    "        yearmonths = [f\"{year}{month:02d}\" for year in years \n",
    "                     for month in months \n",
    "                     if f\"{year}{month:02d}\" >= start_yearmonth \n",
    "                     and f\"{year}{month:02d}\" <= end_yearmonth]\n",
    "        \n",
    "        # Generate future rows more efficiently\n",
    "        future_rows = []\n",
    "        for combo in self.feature_combinations:\n",
    "            state = combo.pop('state')  # Remove state from combo\n",
    "            for ym in yearmonths:\n",
    "                year = ym[:4]\n",
    "                month = ym[4:6]\n",
    "                key = f\"{state}{year[2:]}{month}\"\n",
    "                \n",
    "                row = combo.copy()\n",
    "                row['key'] = key\n",
    "                future_rows.append(row)\n",
    "        \n",
    "        future_data = pd.DataFrame(future_rows)\n",
    "        return future_data\n",
    "    \n",
    "    def predict_future(self, start_yearmonth=202407, end_yearmonth=202412):\n",
    "        \"\"\"Generate and make predictions for future months\"\"\"\n",
    "        # Generate future data\n",
    "        future_data = self.generate_future_data(start_yearmonth, end_yearmonth)\n",
    "        \n",
    "        # Preprocess future data\n",
    "        future_processed = self._preprocess_data(future_data.drop('key', axis=1), fit=False)\n",
    "        future_processed_selected = future_processed[self.important_features]\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = np.expm1(self.model.predict(future_processed_selected))\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results = pd.DataFrame({\n",
    "            'key': future_data['key'],\n",
    "            'predicted_value': predictions\n",
    "        })\n",
    "        \n",
    "        # Add state and date information\n",
    "        results['state'] = results['key'].str[:2]\n",
    "        results['year'] = '20' + results['key'].str[2:4]\n",
    "        results['month'] = results['key'].str[4:6]\n",
    "        results['yearmonth'] = results['year'] + results['month']\n",
    "        \n",
    "        return results.sort_values(['state', 'yearmonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with selected features...\n",
      "[0]\tvalidation_0-rmse:0.00222\n",
      "[1]\tvalidation_0-rmse:0.00219\n",
      "[2]\tvalidation_0-rmse:0.00215\n",
      "[3]\tvalidation_0-rmse:0.00215\n",
      "[4]\tvalidation_0-rmse:0.00211\n",
      "[5]\tvalidation_0-rmse:0.00208\n",
      "[6]\tvalidation_0-rmse:0.00204\n",
      "[7]\tvalidation_0-rmse:0.00201\n",
      "[8]\tvalidation_0-rmse:0.00197\n",
      "[9]\tvalidation_0-rmse:0.00194\n",
      "[10]\tvalidation_0-rmse:0.00191\n",
      "[11]\tvalidation_0-rmse:0.00188\n",
      "[12]\tvalidation_0-rmse:0.00185\n",
      "[13]\tvalidation_0-rmse:0.00185\n",
      "[14]\tvalidation_0-rmse:0.00182\n",
      "[15]\tvalidation_0-rmse:0.00179\n",
      "[16]\tvalidation_0-rmse:0.00176\n",
      "[17]\tvalidation_0-rmse:0.00173\n",
      "[18]\tvalidation_0-rmse:0.00170\n",
      "[19]\tvalidation_0-rmse:0.00167\n",
      "[20]\tvalidation_0-rmse:0.00165\n",
      "[21]\tvalidation_0-rmse:0.00162\n",
      "[22]\tvalidation_0-rmse:0.00159\n",
      "[23]\tvalidation_0-rmse:0.00157\n",
      "[24]\tvalidation_0-rmse:0.00154\n",
      "[25]\tvalidation_0-rmse:0.00152\n",
      "[26]\tvalidation_0-rmse:0.00150\n",
      "[27]\tvalidation_0-rmse:0.00147\n",
      "[28]\tvalidation_0-rmse:0.00145\n",
      "[29]\tvalidation_0-rmse:0.00145\n",
      "[30]\tvalidation_0-rmse:0.00143\n",
      "[31]\tvalidation_0-rmse:0.00141\n",
      "[32]\tvalidation_0-rmse:0.00139\n",
      "[33]\tvalidation_0-rmse:0.00137\n",
      "[34]\tvalidation_0-rmse:0.00134\n",
      "[35]\tvalidation_0-rmse:0.00132\n",
      "[36]\tvalidation_0-rmse:0.00131\n",
      "[37]\tvalidation_0-rmse:0.00129\n",
      "[38]\tvalidation_0-rmse:0.00127\n",
      "[39]\tvalidation_0-rmse:0.00125\n",
      "[40]\tvalidation_0-rmse:0.00123\n",
      "[41]\tvalidation_0-rmse:0.00121\n",
      "[42]\tvalidation_0-rmse:0.00120\n",
      "[43]\tvalidation_0-rmse:0.00118\n",
      "[44]\tvalidation_0-rmse:0.00118\n",
      "[45]\tvalidation_0-rmse:0.00116\n",
      "[46]\tvalidation_0-rmse:0.00115\n",
      "[47]\tvalidation_0-rmse:0.00113\n",
      "[48]\tvalidation_0-rmse:0.00112\n",
      "[49]\tvalidation_0-rmse:0.00110\n",
      "[50]\tvalidation_0-rmse:0.00109\n",
      "[51]\tvalidation_0-rmse:0.00107\n",
      "[52]\tvalidation_0-rmse:0.00106\n",
      "[53]\tvalidation_0-rmse:0.00104\n",
      "[54]\tvalidation_0-rmse:0.00103\n",
      "[55]\tvalidation_0-rmse:0.00102\n",
      "[56]\tvalidation_0-rmse:0.00102\n",
      "[57]\tvalidation_0-rmse:0.00100\n",
      "[58]\tvalidation_0-rmse:0.00099\n",
      "[59]\tvalidation_0-rmse:0.00098\n",
      "[60]\tvalidation_0-rmse:0.00097\n",
      "[61]\tvalidation_0-rmse:0.00096\n",
      "[62]\tvalidation_0-rmse:0.00095\n",
      "[63]\tvalidation_0-rmse:0.00093\n",
      "[64]\tvalidation_0-rmse:0.00092\n",
      "[65]\tvalidation_0-rmse:0.00091\n",
      "[66]\tvalidation_0-rmse:0.00091\n",
      "[67]\tvalidation_0-rmse:0.00091\n",
      "[68]\tvalidation_0-rmse:0.00090\n",
      "[69]\tvalidation_0-rmse:0.00089\n",
      "[70]\tvalidation_0-rmse:0.00088\n",
      "[71]\tvalidation_0-rmse:0.00087\n",
      "[72]\tvalidation_0-rmse:0.00086\n",
      "[73]\tvalidation_0-rmse:0.00085\n",
      "[74]\tvalidation_0-rmse:0.00084\n",
      "[75]\tvalidation_0-rmse:0.00084\n",
      "[76]\tvalidation_0-rmse:0.00084\n",
      "[77]\tvalidation_0-rmse:0.00084\n",
      "[78]\tvalidation_0-rmse:0.00084\n",
      "[79]\tvalidation_0-rmse:0.00084\n",
      "[80]\tvalidation_0-rmse:0.00083\n",
      "[81]\tvalidation_0-rmse:0.00082\n",
      "[82]\tvalidation_0-rmse:0.00081\n",
      "[83]\tvalidation_0-rmse:0.00081\n",
      "[84]\tvalidation_0-rmse:0.00080\n",
      "[85]\tvalidation_0-rmse:0.00079\n",
      "[86]\tvalidation_0-rmse:0.00079\n",
      "[87]\tvalidation_0-rmse:0.00078\n",
      "[88]\tvalidation_0-rmse:0.00077\n",
      "[89]\tvalidation_0-rmse:0.00077\n",
      "[90]\tvalidation_0-rmse:0.00076\n",
      "[91]\tvalidation_0-rmse:0.00076\n",
      "[92]\tvalidation_0-rmse:0.00075\n",
      "[93]\tvalidation_0-rmse:0.00074\n",
      "[94]\tvalidation_0-rmse:0.00074\n",
      "[95]\tvalidation_0-rmse:0.00073\n",
      "[96]\tvalidation_0-rmse:0.00073\n",
      "[97]\tvalidation_0-rmse:0.00072\n",
      "[98]\tvalidation_0-rmse:0.00071\n",
      "[99]\tvalidation_0-rmse:0.00071\n",
      "[100]\tvalidation_0-rmse:0.00070\n",
      "[101]\tvalidation_0-rmse:0.00070\n",
      "[102]\tvalidation_0-rmse:0.00070\n",
      "[103]\tvalidation_0-rmse:0.00069\n",
      "[104]\tvalidation_0-rmse:0.00069\n",
      "[105]\tvalidation_0-rmse:0.00068\n",
      "[106]\tvalidation_0-rmse:0.00068\n",
      "[107]\tvalidation_0-rmse:0.00067\n",
      "[108]\tvalidation_0-rmse:0.00067\n",
      "[109]\tvalidation_0-rmse:0.00066\n",
      "[110]\tvalidation_0-rmse:0.00066\n",
      "[111]\tvalidation_0-rmse:0.00066\n",
      "[112]\tvalidation_0-rmse:0.00065\n",
      "[113]\tvalidation_0-rmse:0.00065\n",
      "[114]\tvalidation_0-rmse:0.00065\n",
      "[115]\tvalidation_0-rmse:0.00064\n",
      "[116]\tvalidation_0-rmse:0.00064\n",
      "[117]\tvalidation_0-rmse:0.00064\n",
      "[118]\tvalidation_0-rmse:0.00063\n",
      "[119]\tvalidation_0-rmse:0.00063\n",
      "[120]\tvalidation_0-rmse:0.00063\n",
      "[121]\tvalidation_0-rmse:0.00063\n",
      "[122]\tvalidation_0-rmse:0.00062\n",
      "[123]\tvalidation_0-rmse:0.00062\n",
      "[124]\tvalidation_0-rmse:0.00062\n",
      "[125]\tvalidation_0-rmse:0.00061\n",
      "[126]\tvalidation_0-rmse:0.00061\n",
      "[127]\tvalidation_0-rmse:0.00061\n",
      "[128]\tvalidation_0-rmse:0.00061\n",
      "[129]\tvalidation_0-rmse:0.00060\n",
      "[130]\tvalidation_0-rmse:0.00060\n",
      "[131]\tvalidation_0-rmse:0.00060\n",
      "[132]\tvalidation_0-rmse:0.00060\n",
      "[133]\tvalidation_0-rmse:0.00059\n",
      "[134]\tvalidation_0-rmse:0.00059\n",
      "[135]\tvalidation_0-rmse:0.00059\n",
      "[136]\tvalidation_0-rmse:0.00059\n",
      "[137]\tvalidation_0-rmse:0.00059\n",
      "[138]\tvalidation_0-rmse:0.00058\n",
      "[139]\tvalidation_0-rmse:0.00058\n",
      "[140]\tvalidation_0-rmse:0.00058\n",
      "[141]\tvalidation_0-rmse:0.00058\n",
      "[142]\tvalidation_0-rmse:0.00058\n",
      "[143]\tvalidation_0-rmse:0.00057\n",
      "[144]\tvalidation_0-rmse:0.00057\n",
      "[145]\tvalidation_0-rmse:0.00057\n",
      "[146]\tvalidation_0-rmse:0.00057\n",
      "[147]\tvalidation_0-rmse:0.00057\n",
      "[148]\tvalidation_0-rmse:0.00057\n",
      "[149]\tvalidation_0-rmse:0.00056\n",
      "[150]\tvalidation_0-rmse:0.00056\n",
      "[151]\tvalidation_0-rmse:0.00056\n",
      "[152]\tvalidation_0-rmse:0.00056\n",
      "[153]\tvalidation_0-rmse:0.00056\n",
      "[154]\tvalidation_0-rmse:0.00056\n",
      "[155]\tvalidation_0-rmse:0.00056\n",
      "[156]\tvalidation_0-rmse:0.00055\n",
      "[157]\tvalidation_0-rmse:0.00055\n",
      "[158]\tvalidation_0-rmse:0.00055\n",
      "[159]\tvalidation_0-rmse:0.00055\n",
      "[160]\tvalidation_0-rmse:0.00055\n",
      "[161]\tvalidation_0-rmse:0.00055\n",
      "[162]\tvalidation_0-rmse:0.00055\n",
      "[163]\tvalidation_0-rmse:0.00055\n",
      "[164]\tvalidation_0-rmse:0.00055\n",
      "[165]\tvalidation_0-rmse:0.00055\n",
      "[166]\tvalidation_0-rmse:0.00055\n",
      "[167]\tvalidation_0-rmse:0.00054\n",
      "[168]\tvalidation_0-rmse:0.00054\n",
      "[169]\tvalidation_0-rmse:0.00054\n",
      "[170]\tvalidation_0-rmse:0.00054\n",
      "[171]\tvalidation_0-rmse:0.00054\n",
      "[172]\tvalidation_0-rmse:0.00054\n",
      "[173]\tvalidation_0-rmse:0.00054\n",
      "[174]\tvalidation_0-rmse:0.00054\n",
      "[175]\tvalidation_0-rmse:0.00054\n",
      "[176]\tvalidation_0-rmse:0.00054\n",
      "[177]\tvalidation_0-rmse:0.00054\n",
      "[178]\tvalidation_0-rmse:0.00054\n",
      "[179]\tvalidation_0-rmse:0.00054\n",
      "[180]\tvalidation_0-rmse:0.00054\n",
      "[181]\tvalidation_0-rmse:0.00053\n",
      "[182]\tvalidation_0-rmse:0.00053\n",
      "[183]\tvalidation_0-rmse:0.00053\n",
      "[184]\tvalidation_0-rmse:0.00053\n",
      "[185]\tvalidation_0-rmse:0.00053\n",
      "[186]\tvalidation_0-rmse:0.00053\n",
      "[187]\tvalidation_0-rmse:0.00053\n",
      "[188]\tvalidation_0-rmse:0.00053\n",
      "[189]\tvalidation_0-rmse:0.00053\n",
      "[190]\tvalidation_0-rmse:0.00053\n",
      "[191]\tvalidation_0-rmse:0.00053\n",
      "[192]\tvalidation_0-rmse:0.00053\n",
      "[193]\tvalidation_0-rmse:0.00053\n",
      "[194]\tvalidation_0-rmse:0.00053\n",
      "[195]\tvalidation_0-rmse:0.00053\n",
      "[196]\tvalidation_0-rmse:0.00053\n",
      "[197]\tvalidation_0-rmse:0.00053\n",
      "[198]\tvalidation_0-rmse:0.00053\n",
      "[199]\tvalidation_0-rmse:0.00053\n",
      "[200]\tvalidation_0-rmse:0.00053\n",
      "[201]\tvalidation_0-rmse:0.00053\n",
      "[202]\tvalidation_0-rmse:0.00053\n",
      "[203]\tvalidation_0-rmse:0.00053\n",
      "[204]\tvalidation_0-rmse:0.00052\n",
      "[205]\tvalidation_0-rmse:0.00052\n",
      "[206]\tvalidation_0-rmse:0.00052\n",
      "[207]\tvalidation_0-rmse:0.00052\n",
      "[208]\tvalidation_0-rmse:0.00052\n",
      "[209]\tvalidation_0-rmse:0.00052\n",
      "[210]\tvalidation_0-rmse:0.00052\n",
      "[211]\tvalidation_0-rmse:0.00052\n",
      "[212]\tvalidation_0-rmse:0.00052\n",
      "[213]\tvalidation_0-rmse:0.00052\n",
      "[214]\tvalidation_0-rmse:0.00052\n",
      "[215]\tvalidation_0-rmse:0.00052\n",
      "[216]\tvalidation_0-rmse:0.00052\n",
      "[217]\tvalidation_0-rmse:0.00052\n",
      "[218]\tvalidation_0-rmse:0.00052\n",
      "[219]\tvalidation_0-rmse:0.00052\n",
      "[220]\tvalidation_0-rmse:0.00052\n",
      "[221]\tvalidation_0-rmse:0.00052\n",
      "[222]\tvalidation_0-rmse:0.00052\n",
      "[223]\tvalidation_0-rmse:0.00052\n",
      "[224]\tvalidation_0-rmse:0.00052\n",
      "[225]\tvalidation_0-rmse:0.00052\n",
      "[226]\tvalidation_0-rmse:0.00052\n",
      "[227]\tvalidation_0-rmse:0.00052\n",
      "[228]\tvalidation_0-rmse:0.00052\n",
      "[229]\tvalidation_0-rmse:0.00052\n",
      "[230]\tvalidation_0-rmse:0.00052\n",
      "[231]\tvalidation_0-rmse:0.00052\n",
      "[232]\tvalidation_0-rmse:0.00052\n",
      "[233]\tvalidation_0-rmse:0.00052\n",
      "[234]\tvalidation_0-rmse:0.00052\n",
      "[235]\tvalidation_0-rmse:0.00052\n",
      "[236]\tvalidation_0-rmse:0.00052\n",
      "[237]\tvalidation_0-rmse:0.00052\n",
      "[238]\tvalidation_0-rmse:0.00052\n",
      "[239]\tvalidation_0-rmse:0.00052\n",
      "[240]\tvalidation_0-rmse:0.00052\n",
      "[241]\tvalidation_0-rmse:0.00052\n",
      "[242]\tvalidation_0-rmse:0.00052\n",
      "[243]\tvalidation_0-rmse:0.00052\n",
      "[244]\tvalidation_0-rmse:0.00052\n",
      "[245]\tvalidation_0-rmse:0.00052\n",
      "[246]\tvalidation_0-rmse:0.00052\n",
      "[247]\tvalidation_0-rmse:0.00052\n",
      "[248]\tvalidation_0-rmse:0.00052\n",
      "[249]\tvalidation_0-rmse:0.00052\n",
      "[250]\tvalidation_0-rmse:0.00052\n",
      "[251]\tvalidation_0-rmse:0.00052\n",
      "[252]\tvalidation_0-rmse:0.00052\n",
      "[253]\tvalidation_0-rmse:0.00052\n",
      "[254]\tvalidation_0-rmse:0.00052\n",
      "[255]\tvalidation_0-rmse:0.00052\n",
      "[256]\tvalidation_0-rmse:0.00052\n",
      "[257]\tvalidation_0-rmse:0.00052\n",
      "[258]\tvalidation_0-rmse:0.00052\n",
      "[259]\tvalidation_0-rmse:0.00052\n",
      "[260]\tvalidation_0-rmse:0.00052\n",
      "[261]\tvalidation_0-rmse:0.00052\n",
      "[262]\tvalidation_0-rmse:0.00052\n",
      "[263]\tvalidation_0-rmse:0.00052\n",
      "[264]\tvalidation_0-rmse:0.00052\n",
      "[265]\tvalidation_0-rmse:0.00052\n",
      "[266]\tvalidation_0-rmse:0.00052\n",
      "[267]\tvalidation_0-rmse:0.00052\n",
      "[268]\tvalidation_0-rmse:0.00052\n",
      "[269]\tvalidation_0-rmse:0.00052\n",
      "[270]\tvalidation_0-rmse:0.00052\n",
      "[271]\tvalidation_0-rmse:0.00052\n",
      "[272]\tvalidation_0-rmse:0.00052\n",
      "[273]\tvalidation_0-rmse:0.00052\n",
      "[274]\tvalidation_0-rmse:0.00052\n",
      "[275]\tvalidation_0-rmse:0.00052\n",
      "[276]\tvalidation_0-rmse:0.00052\n",
      "[277]\tvalidation_0-rmse:0.00052\n",
      "[278]\tvalidation_0-rmse:0.00052\n",
      "[279]\tvalidation_0-rmse:0.00052\n",
      "[280]\tvalidation_0-rmse:0.00052\n",
      "[281]\tvalidation_0-rmse:0.00052\n",
      "[282]\tvalidation_0-rmse:0.00052\n",
      "[283]\tvalidation_0-rmse:0.00052\n",
      "[284]\tvalidation_0-rmse:0.00052\n",
      "[285]\tvalidation_0-rmse:0.00052\n",
      "[286]\tvalidation_0-rmse:0.00052\n",
      "[287]\tvalidation_0-rmse:0.00052\n",
      "[288]\tvalidation_0-rmse:0.00052\n",
      "[289]\tvalidation_0-rmse:0.00052\n",
      "[290]\tvalidation_0-rmse:0.00052\n",
      "[291]\tvalidation_0-rmse:0.00052\n",
      "[292]\tvalidation_0-rmse:0.00052\n",
      "[293]\tvalidation_0-rmse:0.00052\n",
      "[294]\tvalidation_0-rmse:0.00052\n",
      "[295]\tvalidation_0-rmse:0.00052\n",
      "[296]\tvalidation_0-rmse:0.00052\n",
      "[297]\tvalidation_0-rmse:0.00052\n",
      "[298]\tvalidation_0-rmse:0.00052\n",
      "[299]\tvalidation_0-rmse:0.00052\n",
      "[300]\tvalidation_0-rmse:0.00052\n",
      "[301]\tvalidation_0-rmse:0.00052\n",
      "[302]\tvalidation_0-rmse:0.00052\n",
      "[303]\tvalidation_0-rmse:0.00052\n",
      "[304]\tvalidation_0-rmse:0.00052\n",
      "[305]\tvalidation_0-rmse:0.00052\n",
      "[306]\tvalidation_0-rmse:0.00052\n",
      "[307]\tvalidation_0-rmse:0.00052\n",
      "[308]\tvalidation_0-rmse:0.00052\n",
      "[309]\tvalidation_0-rmse:0.00052\n",
      "[310]\tvalidation_0-rmse:0.00052\n",
      "[311]\tvalidation_0-rmse:0.00052\n",
      "[312]\tvalidation_0-rmse:0.00052\n",
      "[313]\tvalidation_0-rmse:0.00052\n",
      "[314]\tvalidation_0-rmse:0.00052\n",
      "[315]\tvalidation_0-rmse:0.00052\n",
      "[316]\tvalidation_0-rmse:0.00052\n",
      "[317]\tvalidation_0-rmse:0.00052\n",
      "[318]\tvalidation_0-rmse:0.00052\n",
      "[319]\tvalidation_0-rmse:0.00052\n",
      "[320]\tvalidation_0-rmse:0.00052\n",
      "[321]\tvalidation_0-rmse:0.00052\n",
      "[322]\tvalidation_0-rmse:0.00052\n",
      "[323]\tvalidation_0-rmse:0.00052\n",
      "[324]\tvalidation_0-rmse:0.00052\n",
      "[325]\tvalidation_0-rmse:0.00052\n",
      "[326]\tvalidation_0-rmse:0.00052\n",
      "[327]\tvalidation_0-rmse:0.00052\n",
      "[328]\tvalidation_0-rmse:0.00052\n",
      "[329]\tvalidation_0-rmse:0.00052\n",
      "[330]\tvalidation_0-rmse:0.00052\n",
      "[331]\tvalidation_0-rmse:0.00052\n",
      "[332]\tvalidation_0-rmse:0.00052\n",
      "[333]\tvalidation_0-rmse:0.00052\n",
      "[334]\tvalidation_0-rmse:0.00052\n",
      "[335]\tvalidation_0-rmse:0.00052\n",
      "[336]\tvalidation_0-rmse:0.00052\n",
      "[337]\tvalidation_0-rmse:0.00052\n",
      "[338]\tvalidation_0-rmse:0.00052\n",
      "[339]\tvalidation_0-rmse:0.00052\n",
      "[340]\tvalidation_0-rmse:0.00052\n",
      "[341]\tvalidation_0-rmse:0.00052\n",
      "[342]\tvalidation_0-rmse:0.00052\n",
      "[343]\tvalidation_0-rmse:0.00052\n",
      "[344]\tvalidation_0-rmse:0.00052\n",
      "[345]\tvalidation_0-rmse:0.00052\n",
      "[346]\tvalidation_0-rmse:0.00052\n",
      "[347]\tvalidation_0-rmse:0.00052\n",
      "[348]\tvalidation_0-rmse:0.00052\n",
      "[349]\tvalidation_0-rmse:0.00052\n",
      "[350]\tvalidation_0-rmse:0.00052\n",
      "[351]\tvalidation_0-rmse:0.00052\n",
      "[352]\tvalidation_0-rmse:0.00052\n",
      "[353]\tvalidation_0-rmse:0.00052\n",
      "[354]\tvalidation_0-rmse:0.00052\n",
      "[355]\tvalidation_0-rmse:0.00052\n",
      "[356]\tvalidation_0-rmse:0.00052\n",
      "[357]\tvalidation_0-rmse:0.00052\n",
      "[358]\tvalidation_0-rmse:0.00052\n",
      "[359]\tvalidation_0-rmse:0.00052\n",
      "[360]\tvalidation_0-rmse:0.00052\n",
      "[361]\tvalidation_0-rmse:0.00052\n",
      "[362]\tvalidation_0-rmse:0.00052\n",
      "[363]\tvalidation_0-rmse:0.00052\n",
      "[364]\tvalidation_0-rmse:0.00052\n",
      "[365]\tvalidation_0-rmse:0.00052\n",
      "[366]\tvalidation_0-rmse:0.00052\n",
      "[367]\tvalidation_0-rmse:0.00052\n",
      "[368]\tvalidation_0-rmse:0.00052\n",
      "[369]\tvalidation_0-rmse:0.00052\n",
      "[370]\tvalidation_0-rmse:0.00052\n",
      "[371]\tvalidation_0-rmse:0.00052\n",
      "[372]\tvalidation_0-rmse:0.00052\n",
      "[373]\tvalidation_0-rmse:0.00052\n",
      "[374]\tvalidation_0-rmse:0.00052\n",
      "[375]\tvalidation_0-rmse:0.00052\n",
      "[376]\tvalidation_0-rmse:0.00052\n",
      "[377]\tvalidation_0-rmse:0.00052\n",
      "[378]\tvalidation_0-rmse:0.00052\n",
      "[379]\tvalidation_0-rmse:0.00052\n",
      "[380]\tvalidation_0-rmse:0.00052\n",
      "[381]\tvalidation_0-rmse:0.00052\n",
      "[382]\tvalidation_0-rmse:0.00052\n",
      "[383]\tvalidation_0-rmse:0.00052\n",
      "[384]\tvalidation_0-rmse:0.00052\n",
      "[385]\tvalidation_0-rmse:0.00052\n",
      "[386]\tvalidation_0-rmse:0.00052\n",
      "[387]\tvalidation_0-rmse:0.00052\n",
      "[388]\tvalidation_0-rmse:0.00052\n",
      "[389]\tvalidation_0-rmse:0.00052\n",
      "[390]\tvalidation_0-rmse:0.00052\n",
      "[391]\tvalidation_0-rmse:0.00052\n",
      "[392]\tvalidation_0-rmse:0.00052\n",
      "[393]\tvalidation_0-rmse:0.00052\n",
      "[394]\tvalidation_0-rmse:0.00052\n",
      "[395]\tvalidation_0-rmse:0.00052\n",
      "[396]\tvalidation_0-rmse:0.00052\n",
      "[397]\tvalidation_0-rmse:0.00052\n",
      "[398]\tvalidation_0-rmse:0.00052\n",
      "[399]\tvalidation_0-rmse:0.00052\n",
      "[400]\tvalidation_0-rmse:0.00052\n",
      "[401]\tvalidation_0-rmse:0.00052\n",
      "[402]\tvalidation_0-rmse:0.00052\n",
      "[403]\tvalidation_0-rmse:0.00052\n",
      "[404]\tvalidation_0-rmse:0.00052\n",
      "[405]\tvalidation_0-rmse:0.00052\n",
      "[406]\tvalidation_0-rmse:0.00052\n",
      "[407]\tvalidation_0-rmse:0.00052\n",
      "[408]\tvalidation_0-rmse:0.00052\n",
      "[409]\tvalidation_0-rmse:0.00052\n",
      "[410]\tvalidation_0-rmse:0.00052\n",
      "[411]\tvalidation_0-rmse:0.00052\n",
      "[412]\tvalidation_0-rmse:0.00052\n",
      "[413]\tvalidation_0-rmse:0.00052\n",
      "[414]\tvalidation_0-rmse:0.00052\n",
      "[415]\tvalidation_0-rmse:0.00052\n",
      "[416]\tvalidation_0-rmse:0.00052\n",
      "[417]\tvalidation_0-rmse:0.00052\n",
      "\n",
      "Training Set Metrics:\n",
      "MSE        3.955170e-07\n",
      "RMSE       6.289015e-04\n",
      "MAE        2.035147e-05\n",
      "R2         9.397678e-01\n",
      "Pearson_R  9.717573e-01\n",
      "\n",
      "Test Set Metrics:\n",
      "MSE        4.086444e-07\n",
      "RMSE       6.392530e-04\n",
      "MAE        2.029701e-05\n",
      "R2         9.343514e-01\n",
      "Pearson_R  9.680166e-01\n",
      "\n",
      "Feature Importance:\n",
      "                        feature  importance\n",
      "10  AnalyticalResultValue_prior    0.290826\n",
      "7                      Region_7    0.096724\n",
      "11           PFASOccurrence_Yes    0.069403\n",
      "13     TreatmentInformation_GWD    0.057209\n",
      "9                      Region_9    0.048081\n",
      "12        DisinfectantType_CLGA    0.047080\n",
      "6                      Region_6    0.041134\n",
      "22         FacilityWaterType_GW    0.039923\n",
      "14             Contaminant_PFBA    0.033122\n",
      "16            Contaminant_PFPeA    0.031396\n",
      "15     PotentialPFASSources_Yes    0.029838\n",
      "19         FacilityWaterType_MX    0.029704\n",
      "23          Contaminant_6:2 FTS    0.025729\n",
      "5                      Region_5    0.024217\n",
      "17             Contaminant_PFBS    0.024204\n",
      "21        DisinfectantType_CAOF    0.023278\n",
      "18        DisinfectantType_CAGC    0.018650\n",
      "20     TreatmentInformation_CON    0.018342\n",
      "4                      Region_4    0.017981\n",
      "3                      Region_3    0.013934\n",
      "8                      Region_8    0.011627\n",
      "2                      Region_2    0.007597\n",
      "1                     Region_10    0.000000\n",
      "0                      Region_1    0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/x0jsj1yd2336l637zvpwcy4h0000gn/T/ipykernel_97467/2317043947.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  'Pearson_R': pearsonr(y_true, y_pred)[0]\n",
      "/var/folders/5c/x0jsj1yd2336l637zvpwcy4h0000gn/T/ipykernel_97467/2317043947.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  'Pearson_R': pearsonr(y_true, y_pred)[0]\n",
      "/var/folders/5c/x0jsj1yd2336l637zvpwcy4h0000gn/T/ipykernel_97467/2317043947.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  'Pearson_R': pearsonr(y_true, y_pred)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contaminant: 11Cl-PF3OUdS Metrics:\n",
      "MSE        1.691019e-11\n",
      "RMSE       4.112200e-06\n",
      "MAE        3.956338e-06\n",
      "R2         0.000000e+00\n",
      "Pearson_R  nan\n",
      "\n",
      "Contaminant: 4:2 FTS Metrics:\n",
      "MSE        3.150714e-11\n",
      "RMSE       5.613122e-06\n",
      "MAE        4.004559e-06\n",
      "R2         9.945241e-01\n",
      "Pearson_R  9.998568e-01\n",
      "\n",
      "Contaminant: 6:2 FTS Metrics:\n",
      "MSE        2.683131e-06\n",
      "RMSE       1.638026e-03\n",
      "MAE        2.559282e-05\n",
      "R2         8.381005e-01\n",
      "Pearson_R  9.171286e-01\n",
      "\n",
      "Contaminant: 8:2 FTS Metrics:\n",
      "MSE        2.977700e-11\n",
      "RMSE       5.456831e-06\n",
      "MAE        3.993574e-06\n",
      "R2         9.946904e-01\n",
      "Pearson_R  9.998714e-01\n",
      "\n",
      "Contaminant: 9Cl-PF3ONS Metrics:\n",
      "MSE        1.767716e-11\n",
      "RMSE       4.204422e-06\n",
      "MAE        3.961295e-06\n",
      "R2         9.279770e-01\n",
      "Pearson_R  9.971286e-01\n",
      "\n",
      "Contaminant: ADONA Metrics:\n",
      "MSE        5.259047e-11\n",
      "RMSE       7.251929e-06\n",
      "MAE        3.970151e-06\n",
      "R2         6.449801e-01\n",
      "Pearson_R  8.846130e-01\n",
      "\n",
      "Contaminant: HFPO-DA Metrics:\n",
      "MSE        8.850053e-10\n",
      "RMSE       2.974904e-05\n",
      "MAE        4.677091e-06\n",
      "R2         9.943250e-01\n",
      "Pearson_R  9.990899e-01\n",
      "\n",
      "Contaminant: NEtFOSAA Metrics:\n",
      "MSE        1.791686e-11\n",
      "RMSE       4.232831e-06\n",
      "MAE        3.965658e-06\n",
      "R2         9.640395e-01\n",
      "Pearson_R  9.986082e-01\n",
      "\n",
      "Contaminant: NFDHA Metrics:\n",
      "MSE        1.594688e-10\n",
      "RMSE       1.262809e-05\n",
      "MAE        4.038135e-06\n",
      "R2         9.962904e-01\n",
      "Pearson_R  9.996511e-01\n",
      "\n",
      "Contaminant: NMeFOSAA Metrics:\n",
      "MSE        1.697194e-11\n",
      "RMSE       4.119701e-06\n",
      "MAE        3.962580e-06\n",
      "R2         0.000000e+00\n",
      "Pearson_R  nan\n",
      "\n",
      "Contaminant: PFBA Metrics:\n",
      "MSE        2.714063e-06\n",
      "RMSE       1.647441e-03\n",
      "MAE        1.270463e-04\n",
      "R2         9.576198e-01\n",
      "Pearson_R  9.831533e-01\n",
      "\n",
      "Contaminant: PFBS Metrics:\n",
      "MSE        1.205564e-06\n",
      "RMSE       1.097982e-03\n",
      "MAE        6.782406e-05\n",
      "R2         9.246687e-01\n",
      "Pearson_R  9.615970e-01\n",
      "\n",
      "Contaminant: PFDA Metrics:\n",
      "MSE        7.069819e-11\n",
      "RMSE       8.408222e-06\n",
      "MAE        4.021999e-06\n",
      "R2         9.830326e-01\n",
      "Pearson_R  9.942798e-01\n",
      "\n",
      "Contaminant: PFDoA Metrics:\n",
      "MSE        1.788906e-11\n",
      "RMSE       4.229547e-06\n",
      "MAE        3.961624e-06\n",
      "R2         9.478589e-01\n",
      "Pearson_R  9.979431e-01\n",
      "\n",
      "Contaminant: PFEESA Metrics:\n",
      "MSE        1.691029e-11\n",
      "RMSE       4.112212e-06\n",
      "MAE        3.956349e-06\n",
      "R2         0.000000e+00\n",
      "Pearson_R  nan\n",
      "\n",
      "Contaminant: PFHpA Metrics:\n",
      "MSE        5.801327e-09\n",
      "RMSE       7.616645e-05\n",
      "MAE        1.210468e-05\n",
      "R2         9.950836e-01\n",
      "Pearson_R  9.988516e-01\n",
      "\n",
      "Contaminant: PFHpS Metrics:\n",
      "MSE        1.856082e-11\n",
      "RMSE       4.308226e-06\n",
      "MAE        3.963460e-06\n",
      "R2         9.692764e-01\n",
      "Pearson_R  9.988246e-01\n",
      "\n",
      "Contaminant: PFHxA Metrics:\n",
      "MSE        1.371259e-07\n",
      "RMSE       3.703051e-04\n",
      "MAE        4.667302e-05\n",
      "R2         9.856673e-01\n",
      "Pearson_R  9.930150e-01\n",
      "\n",
      "Contaminant: PFHxS Metrics:\n",
      "MSE        1.456239e-07\n",
      "RMSE       3.816070e-04\n",
      "MAE        2.084393e-05\n",
      "R2         9.638494e-01\n",
      "Pearson_R  9.851045e-01\n",
      "\n",
      "Contaminant: PFMBA Metrics:\n",
      "MSE        1.928099e-11\n",
      "RMSE       4.391012e-06\n",
      "MAE        3.968429e-06\n",
      "R2         9.662986e-01\n",
      "Pearson_R  9.987433e-01\n",
      "\n",
      "Contaminant: PFMPA Metrics:\n",
      "MSE        3.144480e-11\n",
      "RMSE       5.607566e-06\n",
      "MAE        3.988026e-06\n",
      "R2         9.949742e-01\n",
      "Pearson_R  9.998849e-01\n",
      "\n",
      "Contaminant: PFNA Metrics:\n",
      "MSE        9.187048e-10\n",
      "RMSE       3.031014e-05\n",
      "MAE        4.705103e-06\n",
      "R2         9.946741e-01\n",
      "Pearson_R  9.975362e-01\n",
      "\n",
      "Contaminant: PFOA Metrics:\n",
      "MSE        3.499436e-07\n",
      "RMSE       5.915603e-04\n",
      "MAE        3.482231e-05\n",
      "R2         9.631793e-01\n",
      "Pearson_R  9.909270e-01\n",
      "\n",
      "Contaminant: PFOS Metrics:\n",
      "MSE        1.740095e-07\n",
      "RMSE       4.171445e-04\n",
      "MAE        3.832361e-05\n",
      "R2         9.816199e-01\n",
      "Pearson_R  9.907917e-01\n",
      "\n",
      "Contaminant: PFPeA Metrics:\n",
      "MSE        4.464419e-06\n",
      "RMSE       2.112917e-03\n",
      "MAE        1.351214e-04\n",
      "R2         9.041516e-01\n",
      "Pearson_R  9.512232e-01\n",
      "\n",
      "Contaminant: PFPeS Metrics:\n",
      "MSE        1.435517e-10\n",
      "RMSE       1.198131e-05\n",
      "MAE        4.305028e-06\n",
      "R2         9.976023e-01\n",
      "Pearson_R  9.999671e-01\n",
      "\n",
      "Contaminant: PFTA Metrics:\n",
      "MSE        1.697194e-11\n",
      "RMSE       4.119701e-06\n",
      "MAE        3.962580e-06\n",
      "R2         0.000000e+00\n",
      "Pearson_R  nan\n",
      "\n",
      "Contaminant: PFTrDA Metrics:\n",
      "MSE        1.697168e-11\n",
      "RMSE       4.119670e-06\n",
      "MAE        3.962554e-06\n",
      "R2         0.000000e+00\n",
      "Pearson_R  nan\n",
      "\n",
      "Contaminant: PFUnA Metrics:\n",
      "MSE        9.659356e-11\n",
      "RMSE       9.828202e-06\n",
      "MAE        3.997510e-06\n",
      "R2         8.931288e-01\n",
      "Pearson_R  9.554705e-01\n",
      "\n",
      "Test Set Performance by Contaminant:\n",
      "               count      RMSE        R2\n",
      "contaminant                             \n",
      "11Cl-PF3OUdS  566322  0.000004  0.000000\n",
      "PFEESA        566310  0.000004  0.000000\n",
      "PFTrDA        573330  0.000004  0.000000\n",
      "PFTA          573336  0.000004  0.000000\n",
      "NMeFOSAA      573336  0.000004  0.000000\n",
      "9Cl-PF3ONS    565182  0.000004  0.927977\n",
      "PFDoA         565380  0.000004  0.947859\n",
      "NEtFOSAA      573336  0.000004  0.964039\n",
      "PFHpS         566292  0.000004  0.969276\n",
      "PFMBA         566286  0.000004  0.966299\n",
      "8:2 FTS       565392  0.000005  0.994690\n",
      "PFMPA         566286  0.000006  0.994974\n",
      "4:2 FTS       566232  0.000006  0.994524\n",
      "ADONA         566304  0.000007  0.644980\n",
      "PFDA          566310  0.000008  0.983033\n",
      "PFUnA         566148  0.000010  0.893129\n",
      "PFPeS         566136  0.000012  0.997602\n",
      "NFDHA         566166  0.000013  0.996290\n",
      "HFPO-DA       566250  0.000030  0.994325\n",
      "PFNA          566250  0.000030  0.994674\n",
      "PFHpA         566232  0.000076  0.995084\n",
      "PFHxA         566136  0.000370  0.985667\n",
      "PFHxS         565320  0.000382  0.963849\n",
      "PFOS          566142  0.000417  0.981620\n",
      "PFOA          566100  0.000592  0.963179\n",
      "PFBS          566136  0.001098  0.924669\n",
      "6:2 FTS       565278  0.001638  0.838101\n",
      "PFBA          565998  0.001647  0.957620\n",
      "PFPeA         565050  0.002113  0.904152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/x0jsj1yd2336l637zvpwcy4h0000gn/T/ipykernel_97467/2317043947.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  'Pearson_R': pearsonr(y_true, y_pred)[0]\n",
      "/var/folders/5c/x0jsj1yd2336l637zvpwcy4h0000gn/T/ipykernel_97467/2317043947.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  'Pearson_R': pearsonr(y_true, y_pred)[0]\n"
     ]
    }
   ],
   "source": [
    "predictor = PFASPredictor(feature_threshold=0.02)\n",
    "predictor.fit(tr_x, tr_y, te_x, te_y)\n",
    "\n",
    "# Get detailed evaluation metrics\n",
    "train_metrics, test_metrics = predictor.evaluate_performance(tr_x, tr_y, te_x, te_y)\n",
    "\n",
    "# Get performance by contaminant\n",
    "contam_metrics = predictor.evaluate_by_contaminant(te_x, te_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Save or Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# After training and evaluation\n",
    "save_dir = '/Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/modeling'\n",
    "\n",
    "# Save the predictor object\n",
    "with open(os.path.join(save_dir, 'pfas_predictor.pkl'), 'wb') as f:\n",
    "    pickle.dump(predictor, f)\n",
    "\n",
    "# Save the metrics\n",
    "metrics_dict = {\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'contam_metrics': contam_metrics\n",
    "}\n",
    "with open(os.path.join(save_dir, 'metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(metrics_dict, f)\n",
    "\n",
    "# Load the predictor\n",
    "# with open(os.path.join(save_dir, 'pfas_predictor.pkl'), 'rb') as f:\n",
    "#     predictor = pickle.load(f)\n",
    "\n",
    "# Load the metrics\n",
    "# with open(os.path.join(save_dir, 'metrics.pkl'), 'rb') as f:\n",
    "#     metrics_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict remainder of 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Generate predictions immediately\n",
    "future_predictions = predictor.predict_future(202407, 202412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Save or Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Load the saved predictor and metrics\n",
    "save_dir = '/Users/kb/Library/CloudStorage/OneDrive-Personal/projects/cs_6463_final/modeling'\n",
    "\n",
    "# Save the future predictions\n",
    "# with open(os.path.join(save_dir, 'future_predictions.pkl'), 'wb') as f:\n",
    "#     pickle.dump(future_predictions, f)\n",
    "\n",
    "# Load the future predictions\n",
    "with open(os.path.join(save_dir, 'future_predictions.pkl'), 'rb') as f:\n",
    "    future_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Regional Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regional Prediction Summary:\n",
      "        count      mean       std       min       max    median\n",
      "state                                                          \n",
      "GU      29238  0.006212  0.013225  0.000003  0.159319  0.003694\n",
      "MP      30888  0.004748  0.018869  0.000003  0.143745  0.000003\n",
      "MN     139524  0.004638  0.025404  0.000003  0.609702  0.000003\n",
      "DE     128040  0.003363  0.014317  0.000003  0.149935  0.000003\n",
      "01      11022  0.002501  0.004565  0.000003  0.018281  0.000003\n",
      "...       ...       ...       ...       ...       ...       ...\n",
      "05      19668  0.000008  0.000014  0.000003  0.000071  0.000003\n",
      "02       3828  0.000007  0.000012  0.000003  0.000053  0.000003\n",
      "10       9570  0.000007  0.000012  0.000003  0.000053  0.000003\n",
      "08      28710  0.000007  0.000012  0.000003  0.000054  0.000003\n",
      "VI       5742  0.000007  0.000012  0.000003  0.000053  0.000003\n",
      "\n",
      "[65 rows x 6 columns]\n",
      "\n",
      "Monthly Trends by Region:\n",
      "yearmonth    202407    202408    202409    202410    202411    202412\n",
      "state                                                                \n",
      "01         0.002501  0.002501  0.002501  0.002501  0.002501  0.002501\n",
      "02         0.000007  0.000007  0.000007  0.000007  0.000007  0.000007\n",
      "04         0.000011  0.000011  0.000011  0.000011  0.000011  0.000011\n",
      "05         0.000008  0.000008  0.000008  0.000008  0.000008  0.000008\n",
      "06         0.000026  0.000026  0.000026  0.000026  0.000026  0.000026\n",
      "...             ...       ...       ...       ...       ...       ...\n",
      "VT         0.000015  0.000015  0.000015  0.000015  0.000015  0.000015\n",
      "WA         0.000355  0.000355  0.000355  0.000355  0.000355  0.000355\n",
      "WI         0.000232  0.000232  0.000232  0.000232  0.000232  0.000232\n",
      "WV         0.000469  0.000469  0.000469  0.000469  0.000469  0.000469\n",
      "WY         0.000081  0.000081  0.000081  0.000081  0.000081  0.000081\n",
      "\n",
      "[65 rows x 6 columns]\n",
      "\n",
      "Detailed Regional Prediction Summary:\n",
      "   state  total_predictions  mean_value       trend  volatility highest_month  \\\n",
      "20    GU              29238    0.006212  Decreasing         0.0        202407   \n",
      "35    MP              30888    0.004748  Decreasing         0.0        202407   \n",
      "33    MN             139524    0.004638  Decreasing         0.0        202407   \n",
      "17    DE             128040    0.003363  Decreasing         0.0        202407   \n",
      "0     01              11022    0.002501  Decreasing         0.0        202407   \n",
      "..   ...                ...         ...         ...         ...           ...   \n",
      "3     05              19668    0.000008  Decreasing         0.0        202407   \n",
      "5     08              28710    0.000007  Decreasing         0.0        202407   \n",
      "1     02               3828    0.000007  Decreasing         0.0        202407   \n",
      "7     10               9570    0.000007  Decreasing         0.0        202407   \n",
      "59    VI               5742    0.000007  Decreasing         0.0        202407   \n",
      "\n",
      "   lowest_month  \n",
      "20       202407  \n",
      "35       202407  \n",
      "33       202407  \n",
      "17       202407  \n",
      "0        202407  \n",
      "..          ...  \n",
      "3        202407  \n",
      "5        202407  \n",
      "1        202407  \n",
      "7        202407  \n",
      "59       202407  \n",
      "\n",
      "[65 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "def analyze_future_predictions_by_region(future_predictions):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of predicted PFAS levels by region\n",
    "    \n",
    "    Parameters:\n",
    "    future_predictions: DataFrame containing the predictions\n",
    "    \"\"\"\n",
    "    # Add descriptive statistics for each region\n",
    "    regional_stats = (future_predictions\n",
    "        .groupby('state')\n",
    "        .agg({\n",
    "            'predicted_value': ['count', 'mean', 'std', 'min', 'max', 'median'],\n",
    "        })\n",
    "        .round(6)\n",
    "    )\n",
    "    \n",
    "    # Flatten column names\n",
    "    regional_stats.columns = ['count', 'mean', 'std', 'min', 'max', 'median']\n",
    "    \n",
    "    # Sort by mean predicted value\n",
    "    regional_stats_sorted = regional_stats.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nRegional Prediction Summary:\")\n",
    "    print(regional_stats_sorted)\n",
    "    \n",
    "    # Monthly trends by region\n",
    "    monthly_trends = (future_predictions\n",
    "        .groupby(['state', 'yearmonth'])\n",
    "        .agg({\n",
    "            'predicted_value': 'mean'\n",
    "        })\n",
    "        .round(6)\n",
    "        .reset_index()\n",
    "        .pivot(index='state', columns='yearmonth', values='predicted_value')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMonthly Trends by Region:\")\n",
    "    print(monthly_trends)\n",
    "    \n",
    "    return regional_stats_sorted, monthly_trends\n",
    "\n",
    "# Optional: Create a more detailed summary of predictions\n",
    "def get_detailed_regional_summary(future_predictions):\n",
    "    \"\"\"Get detailed statistics about predictions by region and month\"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for state in future_predictions['state'].unique():\n",
    "        state_data = future_predictions[future_predictions['state'] == state]\n",
    "        \n",
    "        # Calculate trend\n",
    "        monthly_means = state_data.groupby('yearmonth')['predicted_value'].mean()\n",
    "        trend = 'Increasing' if monthly_means.iloc[-1] > monthly_means.iloc[0] else 'Decreasing'\n",
    "        \n",
    "        # Calculate volatility (standard deviation of month-to-month changes)\n",
    "        volatility = monthly_means.pct_change().std()\n",
    "        \n",
    "        summary.append({\n",
    "            'state': state,\n",
    "            'total_predictions': len(state_data),\n",
    "            'mean_value': state_data['predicted_value'].mean(),\n",
    "            'trend': trend,\n",
    "            'volatility': volatility,\n",
    "            'highest_month': monthly_means.idxmax(),\n",
    "            'lowest_month': monthly_means.idxmin(),\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary).sort_values('mean_value', ascending=False)\n",
    "    print(\"\\nDetailed Regional Prediction Summary:\")\n",
    "    print(summary_df)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Execute the analysis\n",
    "regional_stats, monthly_trends = analyze_future_predictions_by_region(future_predictions)\n",
    "\n",
    "# Get detailed summary\n",
    "detailed_summary = get_detailed_regional_summary(future_predictions)\n",
    "\n",
    "# Export to csv\n",
    "detailed_summary.to_csv(os.path.join(save_dir, 'detailed_summary.csv'), index=False)\n",
    "regional_stats.to_csv(os.path.join(save_dir, 'regional_stats.csv'))\n",
    "monthly_trends.to_csv(os.path.join(save_dir, 'monthly_trends.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ds_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
